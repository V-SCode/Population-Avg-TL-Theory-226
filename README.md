# Population Avg TL Theory 226
Initial code base for a population averaging theory developed initially for a theoretical and computational neuroscience course

Working abstract: 

We study elastic transfer learning in infinite-width feature-learning networks, building on the adaptive-kernel framework of Lauditi, Bordelon, and Pehlevan. In the
linear teacher–student setting with isotropic Gaussian inputs, the transfer dynamics
admit a dataset-conditioned saddle-point description in terms of an adapted feature kernel. We then propose a data-averaging program: by combining low-rank
order-parameter reductions of the adaptive kernel with Marchenko–Pastur deterministic equivalents for ridge regression, we obtain an explicit population-level
proxy for target generalization and transfer gain as a function of task similarity,
sample budgets, feature-learning strength, and elastic coupling. This yields testable
predictions for transfer phase boundaries and motivates spectral and geometric
diagnostics for feature reuse beyond the linear regime.
